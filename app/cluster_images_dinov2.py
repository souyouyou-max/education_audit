#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆèšç±»è„šæœ¬ v3: DINOv2 ç‰¹å¾ + é¢œè‰²ç‰¹å¾èåˆ
- æ ¸å¿ƒé—®é¢˜ï¼š4.png åœ¨ DINOv2 è¯­ä¹‰ç©ºé—´ä¸ 1,2,3 å¾ˆè¿‘ï¼Œä½†é¢œè‰²ä¸åŒ
- è§£å†³æ–¹æ¡ˆï¼šèåˆé¢œè‰²ç›´æ–¹å›¾ç‰¹å¾ï¼Œå¢å¤§é¢œè‰²å·®å¼‚çš„å½±å“
- 14.pngï¼šè‡ªç„¶è½åœ¨å™ªå£°ä¸­

äººå·¥æ ‡æ³¨:
- ç»„A: 1,2,3ï¼›ç»„B: 5,6,7ï¼›ç»„C: 8,9ï¼›ç»„D: 10,11ï¼›ç»„E: 12,13ï¼›å•ç‚¹: 4,14
"""

import argparse
import os
import shutil
from pathlib import Path
from collections import defaultdict

import torch
from PIL import Image
import numpy as np
from tqdm import tqdm
from transformers import AutoImageProcessor, AutoModel
import hdbscan
from sklearn.preprocessing import normalize, StandardScaler
import cv2


def parse_args():
    parser = argparse.ArgumentParser(description="æ¯•ä¸šè¯æ¨¡æ¿åˆ†ç»„ï¼ˆDINOv2+é¢œè‰²èåˆç‰ˆï¼‰")
    parser.add_argument("--image_dir", type=str, default="pic/",
                        help="å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--model", type=str, default="facebook/dinov2-large",
                        help="DINOv2 æ¨¡å‹")
    parser.add_argument("--batch_size", type=int, default=4,
                        help="æ‰¹å¤§å°")
    parser.add_argument("--min_cluster_size", type=int, default=2,
                        help="æœ€å°ç°‡å¤§å°ï¼ˆâ‰¥2ï¼‰")
    parser.add_argument("--epsilon", type=float, default=0.03,
                        help="HDBSCAN epsilonï¼ˆ0.03 ä¸ºæœ€ä¼˜å€¼ï¼Œå¯¹åº” 1-14.png å®Œå…¨æ­£ç¡®ï¼‰")
    parser.add_argument("--color_weight", type=float, default=0.4,
                        help="é¢œè‰²ç‰¹å¾æƒé‡ï¼ˆ0=çº¯DINOv2ï¼Œ1=çº¯é¢œè‰²ï¼‰ã€‚0.4 èƒ½åŒºåˆ†4.pngä¸ç»„A")
    parser.add_argument("--output_dir", type=str, default="clustered_output",
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--action", choices=["copy", "move"], default="copy",
                        help="copy æˆ– move")
    return parser.parse_args()


def extract_color_features(img_path):
    """æå–é¢œè‰²ç‰¹å¾ï¼šLABé¢œè‰²ç›´æ–¹å›¾ + å‡å€¼ + çº¢å°æ¯”ä¾‹"""
    try:
        img = cv2.imread(str(img_path))
        if img is None:
            return np.zeros(64)
        
        # è½¬ä¸º LAB é¢œè‰²ç©ºé—´
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        
        # Lé€šé“ç›´æ–¹å›¾ï¼ˆäº®åº¦ï¼‰
        hist_l = cv2.calcHist([lab], [0], None, [16], [0, 256]).flatten()
        # Aé€šé“ç›´æ–¹å›¾ï¼ˆçº¢ç»¿ï¼‰
        hist_a = cv2.calcHist([lab], [1], None, [16], [0, 256]).flatten()
        # Bé€šé“ç›´æ–¹å›¾ï¼ˆé»„è“ï¼‰
        hist_b = cv2.calcHist([lab], [2], None, [16], [0, 256]).flatten()
        
        # å½’ä¸€åŒ–ç›´æ–¹å›¾
        hist_l = hist_l / (hist_l.sum() + 1e-8)
        hist_a = hist_a / (hist_a.sum() + 1e-8)
        hist_b = hist_b / (hist_b.sum() + 1e-8)
        
        # å‡å€¼é¢œè‰²ï¼ˆBGR -> LABå‡å€¼ï¼‰
        mean_lab = lab.mean(axis=(0,1)) / 255.0
        
        # çº¢å°æ¯”ä¾‹
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        mask_r1 = cv2.inRange(hsv, np.array([0,70,50]), np.array([10,255,255]))
        mask_r2 = cv2.inRange(hsv, np.array([170,70,50]), np.array([180,255,255]))
        red_ratio = np.sum((mask_r1 + mask_r2) > 0) / (img.shape[0] * img.shape[1])
        
        features = np.concatenate([hist_l, hist_a, hist_b, mean_lab, [red_ratio]])
        return features
    except Exception as e:
        return np.zeros(52)


def main():
    args = parse_args()

    image_dir = Path(args.image_dir).expanduser().resolve()
    if not image_dir.is_dir():
        print(f"é”™è¯¯ï¼šæ–‡ä»¶å¤¹ä¸å­˜åœ¨ -> {image_dir}")
        return

    output_base = Path(args.output_dir).expanduser().resolve()
    output_base.mkdir(exist_ok=True, parents=True)

    exts = ("*.jpg", "*.jpeg", "*.JPG", "*.JPEG", "*.png", "*.PNG")
    image_paths = sorted(p for ext in exts for p in image_dir.rglob(ext))
    print(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾ç‰‡")

    # åˆ¤æ–­è®¾å¤‡ï¼ˆä¼˜å…ˆ MPSï¼‰
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    print(f"åŠ è½½æ¨¡å‹ï¼š{args.model}")
    processor = AutoImageProcessor.from_pretrained(args.model)
    model_nn = AutoModel.from_pretrained(args.model).to(device).eval()

    # æå– DINOv2 CLS token ç‰¹å¾
    def extract_dino_features(paths, bs):
        feats_list = []
        for start in tqdm(range(0, len(paths), bs), desc="æå–DINOv2ç‰¹å¾"):
            batch = paths[start:start + bs]
            try:
                images = [Image.open(p).convert("RGB") for p in batch]
            except Exception as e:
                print(f"è·³è¿‡æŸåå›¾ç‰‡ï¼š{e}")
                feats_list.append(np.zeros((len(batch), 1024)))
                continue
            inputs = processor(images=images, return_tensors="pt").to(device)
            with torch.no_grad():
                outputs = model_nn(**inputs)
                feat = outputs.last_hidden_state[:, 0]  # CLS token
            feats_list.append(feat.cpu().float().numpy())
        if not feats_list:
            return np.array([])
        return normalize(np.concatenate(feats_list))

    print("å¼€å§‹æå– DINOv2 ç‰¹å¾...")
    dino_embeddings = extract_dino_features(image_paths, args.batch_size)
    if len(dino_embeddings) == 0:
        return
    print(f"DINOv2ç‰¹å¾å½¢çŠ¶ï¼š{dino_embeddings.shape}")

    # æå–é¢œè‰²ç‰¹å¾
    print("æå–é¢œè‰²ç‰¹å¾...")
    color_features = []
    for p in tqdm(image_paths, desc="é¢œè‰²ç‰¹å¾"):
        color_features.append(extract_color_features(p))
    color_features = np.array(color_features)
    
    # å½’ä¸€åŒ–é¢œè‰²ç‰¹å¾
    scaler = StandardScaler()
    color_norm = normalize(scaler.fit_transform(color_features))
    print(f"é¢œè‰²ç‰¹å¾å½¢çŠ¶ï¼š{color_norm.shape}")

    # èåˆç‰¹å¾ï¼ˆåŠ æƒç»„åˆï¼‰
    # è°ƒæ•´æƒé‡ä½¿é¢œè‰²èƒ½æœ‰æ•ˆåŒºåˆ† 4.png å’Œ 1,2,3
    dino_weight = 1.0 - args.color_weight
    color_weight = args.color_weight
    
    combined = normalize(np.concatenate([
        dino_embeddings * dino_weight,
        color_norm * color_weight
    ], axis=1))
    print(f"èåˆç‰¹å¾å½¢çŠ¶ï¼š{combined.shape}")

    # è®¡ç®—å…³é”®å›¾ç‰‡çš„è·ç¦»å¹¶éªŒè¯
    from scipy.spatial.distance import pdist, squareform
    dist_full = squareform(pdist(combined, metric='euclidean'))
    
    key_names = [f"{i}.png" for i in range(1, 15)]
    key_indices = {}
    for idx, p in enumerate(image_paths):
        if p.name in key_names:
            key_indices[p.name] = idx
    
    if len(key_indices) == 14:
        print("\n--- èåˆç‰¹å¾å…³é”®è·ç¦» ---")
        def kd(a, b):
            return dist_full[key_indices[a], key_indices[b]]
        
        print(f"ç»„Aå†…: dist(1,2)={kd('1.png','2.png'):.4f}, dist(1,3)={kd('1.png','3.png'):.4f}, dist(2,3)={kd('2.png','3.png'):.4f}")
        print(f"4åˆ°A: dist(4,1)={kd('4.png','1.png'):.4f}, dist(4,2)={kd('4.png','2.png'):.4f}, dist(4,3)={kd('4.png','3.png'):.4f}")
        print(f"ç»„Bå†…: dist(5,6)={kd('5.png','6.png'):.4f}, dist(5,7)={kd('5.png','7.png'):.4f}, dist(6,7)={kd('6.png','7.png'):.4f}")
        print(f"ç»„B-D: dist(6,10)={kd('6.png','10.png'):.4f}, dist(5,13)={kd('5.png','13.png'):.4f}")
        print(f"14åˆ°E: dist(14,12)={kd('14.png','12.png'):.4f}, dist(14,13)={kd('14.png','13.png'):.4f}")
        print(f"ç»„Då†…: dist(10,11)={kd('10.png','11.png'):.4f}")
        print(f"ç»„Eå†…: dist(12,13)={kd('12.png','13.png'):.4f}")
        print(f"ç»„Cå†…: dist(8,9)={kd('8.png','9.png'):.4f}")

    # å…¨å±€ HDBSCAN èšç±»
    print(f"\nè¿è¡Œ HDBSCAN (epsilon={args.epsilon}, min_cluster_size={args.min_cluster_size})...")
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=args.min_cluster_size,
        min_samples=1,
        metric='euclidean',
        cluster_selection_epsilon=args.epsilon,
        cluster_selection_method='eom',
    )
    labels = clusterer.fit_predict(combined)
    
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = np.sum(labels == -1)
    print(f"å‘ç° {n_clusters} ä¸ªç°‡ï¼Œ{n_noise} ä¸ªå™ªå£°ç‚¹")

    # æ£€éªŒå…³é”®å›¾ç‰‡åˆ†ç»„
    if len(key_indices) > 0:
        sorted_keys = sorted(key_indices.keys(), key=lambda x: int(x.replace('.png', '')))
        print("\n--- å…³é”®å›¾ç‰‡ 1-14.png åˆ†ç»„ç»“æœ ---")
        for name in sorted_keys:
            if name in key_indices:
                idx = key_indices[name]
                lbl = labels[idx]
                print(f"  {name:10} ç°‡ {lbl:4d}")
        
        # éªŒè¯
        print("\n--- å¯¹æ¯”äººå·¥æ ‡æ³¨ ---")
        expected = {
            'ç»„A': ['1.png', '2.png', '3.png'],
            'ç»„B': ['5.png', '6.png', '7.png'],
            'ç»„C': ['8.png', '9.png'],
            'ç»„D': ['10.png', '11.png'],
            'ç»„E': ['12.png', '13.png'],
        }
        singletons = ['4.png', '14.png']
        
        all_correct = True
        for group_name, members in expected.items():
            member_labels = [labels[key_indices[m]] for m in members if m in key_indices]
            is_ok = len(set(member_labels)) == 1 and member_labels[0] != -1
            lbl_val = member_labels[0] if member_labels else None
            if is_ok:
                # Check no contamination from other key images
                same_cluster_keys = [n for n in sorted_keys if n in key_indices 
                                      and labels[key_indices[n]] == lbl_val 
                                      and n not in members]
                if same_cluster_keys:
                    print(f"  âš ï¸ {group_name}: æ ‡ç­¾={lbl_val}ï¼ŒåŒ…å«é¢å¤–å…³é”®å›¾ç‰‡ {same_cluster_keys}")
                    all_correct = False
                else:
                    print(f"  âœ… {group_name}: æ ‡ç­¾={lbl_val}")
            else:
                print(f"  âŒ {group_name}: åˆ†ç»„å¤±è´¥ï¼Œæ ‡ç­¾={member_labels}")
                all_correct = False
        
        for s in singletons:
            if s not in key_indices:
                continue
            lbl = labels[key_indices[s]]
            same_as = [n for n in sorted_keys if n in key_indices 
                       and labels[key_indices[n]] == lbl and n != s]
            if not same_as:
                print(f"  âœ… å•ç‚¹{s}: æ ‡ç­¾={lbl}ï¼ˆç‹¬ç«‹ï¼‰")
            else:
                print(f"  âŒ å•ç‚¹{s}: æ ‡ç­¾={lbl}ï¼Œä¸ {same_as} åŒç°‡ï¼ˆåº”ç‹¬ç«‹ï¼‰")
                all_correct = False
        
        if all_correct:
            print("\nğŸ‰ æ‰€æœ‰åˆ†ç»„ä¸äººå·¥æ ‡æ³¨å®Œå…¨ä¸€è‡´ï¼")
        else:
            print("\nâš ï¸ è¿˜æœ‰åˆ†ç»„ä¸ä¸€è‡´")

    # ä¿å­˜åˆ†ç»„ç»“æœ
    print(f"\nä¿å­˜ç»“æœåˆ° {output_base}...")
    groups = defaultdict(list)
    for path, label in zip(image_paths, labels):
        groups[label].append(path)

    for label, paths in sorted(groups.items(), key=lambda x: -len(x[1])):
        if label == -1:
            cluster_dir = output_base / "unique_or_noise"
            title = f"ç‹¬ç‰¹/å™ªå£° ({len(paths)} å¼ )"
        else:
            cluster_dir = output_base / f"template_{label:02d}"
            title = f"æ¨¡æ¿ç¾¤ {label} ({len(paths)} å¼ )"

        cluster_dir.mkdir(exist_ok=True)
        print(f"\n{title}")
        for p in paths:
            dest = cluster_dir / p.name
            if args.action == "copy":
                shutil.copy2(p, dest)
            else:
                shutil.move(p, dest)
            print(f"  {p.name}")

    print(f"\nåˆ†ç»„å®Œæˆï¼ç»“æœåœ¨ï¼š{output_base}")


if __name__ == "__main__":
    main()
